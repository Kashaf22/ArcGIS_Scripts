{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kashaf22/ArcGIS_Scripts/blob/main/InUse_LSTM_Model3_Updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbwoVWDk-9fi",
        "outputId": "1ffe71c1-6573-41f3-8b47-4c569c4a2c5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "19/19 [==============================] - 11s 258ms/step - loss: 66266.1875 - val_loss: 68390.6797\n",
            "Epoch 2/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 47550.1602 - val_loss: 50576.7188\n",
            "Epoch 3/200\n",
            "19/19 [==============================] - 2s 88ms/step - loss: 34633.7461 - val_loss: 38285.8438\n",
            "Epoch 4/200\n",
            "19/19 [==============================] - 2s 88ms/step - loss: 25942.5996 - val_loss: 29330.6113\n",
            "Epoch 5/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 19653.9219 - val_loss: 22997.4336\n",
            "Epoch 6/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 15210.1729 - val_loss: 18261.3281\n",
            "Epoch 7/200\n",
            "19/19 [==============================] - 3s 141ms/step - loss: 12002.9873 - val_loss: 14741.5391\n",
            "Epoch 8/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 9690.0645 - val_loss: 12178.2500\n",
            "Epoch 9/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 8020.9883 - val_loss: 10186.5361\n",
            "Epoch 10/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 6760.6196 - val_loss: 8652.3066\n",
            "Epoch 11/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 5764.4512 - val_loss: 7470.6978\n",
            "Epoch 12/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 4982.8047 - val_loss: 6429.4893\n",
            "Epoch 13/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 4328.6636 - val_loss: 5592.3159\n",
            "Epoch 14/200\n",
            "19/19 [==============================] - 2s 130ms/step - loss: 3787.5857 - val_loss: 4884.6245\n",
            "Epoch 15/200\n",
            "19/19 [==============================] - 2s 104ms/step - loss: 3316.7070 - val_loss: 4325.5874\n",
            "Epoch 16/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 2919.4265 - val_loss: 3811.3206\n",
            "Epoch 17/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 2570.0649 - val_loss: 3391.5425\n",
            "Epoch 18/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 2265.7412 - val_loss: 3003.4426\n",
            "Epoch 19/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 1962.8350 - val_loss: 2413.3257\n",
            "Epoch 20/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 1522.7465 - val_loss: 2068.0298\n",
            "Epoch 21/200\n",
            "19/19 [==============================] - 2s 119ms/step - loss: 1331.2794 - val_loss: 1778.9364\n",
            "Epoch 22/200\n",
            "19/19 [==============================] - 2s 109ms/step - loss: 1156.7543 - val_loss: 1549.6239\n",
            "Epoch 23/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 1005.3013 - val_loss: 1366.5706\n",
            "Epoch 24/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 893.8135 - val_loss: 1231.8932\n",
            "Epoch 25/200\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 811.0287 - val_loss: 1116.0370\n",
            "Epoch 26/200\n",
            "19/19 [==============================] - 3s 137ms/step - loss: 741.8060 - val_loss: 1019.7300\n",
            "Epoch 27/200\n",
            "19/19 [==============================] - 2s 91ms/step - loss: 687.0972 - val_loss: 939.8234\n",
            "Epoch 28/200\n",
            "19/19 [==============================] - 3s 133ms/step - loss: 636.5784 - val_loss: 875.6365\n",
            "Epoch 29/200\n",
            "19/19 [==============================] - 2s 81ms/step - loss: 595.0717 - val_loss: 818.1960\n",
            "Epoch 30/200\n",
            "19/19 [==============================] - 2s 81ms/step - loss: 557.7374 - val_loss: 764.3141\n",
            "Epoch 31/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 520.2552 - val_loss: 715.5141\n",
            "Epoch 32/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 486.8986 - val_loss: 679.2247\n",
            "Epoch 33/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 459.1327 - val_loss: 651.2590\n",
            "Epoch 34/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 431.7022 - val_loss: 595.7010\n",
            "Epoch 35/200\n",
            "19/19 [==============================] - 3s 140ms/step - loss: 405.5464 - val_loss: 571.8547\n",
            "Epoch 36/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 384.2488 - val_loss: 529.7782\n",
            "Epoch 37/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 361.3258 - val_loss: 504.9305\n",
            "Epoch 38/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 350.5040 - val_loss: 492.4766\n",
            "Epoch 39/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 326.9753 - val_loss: 463.0696\n",
            "Epoch 40/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 304.8980 - val_loss: 427.1886\n",
            "Epoch 41/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 288.0094 - val_loss: 401.5320\n",
            "Epoch 42/200\n",
            "19/19 [==============================] - 2s 122ms/step - loss: 270.6094 - val_loss: 375.3441\n",
            "Epoch 43/200\n",
            "19/19 [==============================] - 2s 100ms/step - loss: 254.1042 - val_loss: 359.7590\n",
            "Epoch 44/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 239.3263 - val_loss: 337.4079\n",
            "Epoch 45/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 225.2764 - val_loss: 322.9995\n",
            "Epoch 46/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 211.1985 - val_loss: 298.2060\n",
            "Epoch 47/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 199.3244 - val_loss: 286.4338\n",
            "Epoch 48/200\n",
            "19/19 [==============================] - 2s 89ms/step - loss: 185.7400 - val_loss: 269.0983\n",
            "Epoch 49/200\n",
            "19/19 [==============================] - 2s 122ms/step - loss: 176.8846 - val_loss: 264.9998\n",
            "Epoch 50/200\n",
            "19/19 [==============================] - 2s 105ms/step - loss: 178.4642 - val_loss: 232.2365\n",
            "Epoch 51/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 159.8757 - val_loss: 225.1451\n",
            "Epoch 52/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 153.7765 - val_loss: 216.6708\n",
            "Epoch 53/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 143.0800 - val_loss: 200.8014\n",
            "Epoch 54/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 133.6259 - val_loss: 191.0382\n",
            "Epoch 55/200\n",
            "19/19 [==============================] - 2s 105ms/step - loss: 136.2793 - val_loss: 191.6340\n",
            "Epoch 56/200\n",
            "19/19 [==============================] - 2s 117ms/step - loss: 125.3716 - val_loss: 172.3630\n",
            "Epoch 57/200\n",
            "19/19 [==============================] - 2s 111ms/step - loss: 120.5009 - val_loss: 173.8589\n",
            "Epoch 58/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 116.5532 - val_loss: 159.1064\n",
            "Epoch 59/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 109.6880 - val_loss: 151.0991\n",
            "Epoch 60/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 102.2302 - val_loss: 143.4272\n",
            "Epoch 61/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 99.4380 - val_loss: 133.5343\n",
            "Epoch 62/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 95.3714 - val_loss: 134.5403\n",
            "Epoch 63/200\n",
            "19/19 [==============================] - 2s 108ms/step - loss: 91.6783 - val_loss: 132.2444\n",
            "Epoch 64/200\n",
            "19/19 [==============================] - 2s 118ms/step - loss: 92.6203 - val_loss: 123.2154\n",
            "Epoch 65/200\n",
            "19/19 [==============================] - 2s 96ms/step - loss: 84.0643 - val_loss: 117.1812\n",
            "Epoch 66/200\n",
            "19/19 [==============================] - 2s 91ms/step - loss: 80.1285 - val_loss: 107.4250\n",
            "Epoch 67/200\n",
            "19/19 [==============================] - 2s 91ms/step - loss: 73.6731 - val_loss: 104.7267\n",
            "Epoch 68/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 75.8020 - val_loss: 106.9684\n",
            "Epoch 69/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 72.6653 - val_loss: 98.7425\n",
            "Epoch 70/200\n",
            "19/19 [==============================] - 2s 120ms/step - loss: 65.9591 - val_loss: 92.9443\n",
            "Epoch 71/200\n",
            "19/19 [==============================] - 2s 105ms/step - loss: 70.6134 - val_loss: 94.0029\n",
            "Epoch 72/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 63.8121 - val_loss: 93.0813\n",
            "Epoch 73/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 58.8131 - val_loss: 87.9198\n",
            "Epoch 74/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 57.1533 - val_loss: 85.4512\n",
            "Epoch 75/200\n",
            "19/19 [==============================] - 2s 81ms/step - loss: 57.5023 - val_loss: 73.5445\n",
            "Epoch 76/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 52.0818 - val_loss: 71.6147\n",
            "Epoch 77/200\n",
            "19/19 [==============================] - 2s 118ms/step - loss: 48.9548 - val_loss: 68.7772\n",
            "Epoch 78/200\n",
            "19/19 [==============================] - 2s 119ms/step - loss: 47.2035 - val_loss: 65.9049\n",
            "Epoch 79/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 45.7177 - val_loss: 64.6916\n",
            "Epoch 80/200\n",
            "19/19 [==============================] - 2s 92ms/step - loss: 43.9783 - val_loss: 61.7958\n",
            "Epoch 81/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 42.5208 - val_loss: 60.2954\n",
            "Epoch 82/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 41.2368 - val_loss: 57.2957\n",
            "Epoch 83/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 40.1403 - val_loss: 56.8582\n",
            "Epoch 84/200\n",
            "19/19 [==============================] - 2s 116ms/step - loss: 38.9815 - val_loss: 55.3121\n",
            "Epoch 85/200\n",
            "19/19 [==============================] - 2s 113ms/step - loss: 37.8263 - val_loss: 54.0409\n",
            "Epoch 86/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 36.9429 - val_loss: 52.2145\n",
            "Epoch 87/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 35.9515 - val_loss: 51.3287\n",
            "Epoch 88/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 39.9346 - val_loss: 54.1781\n",
            "Epoch 89/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 41.8664 - val_loss: 51.1088\n",
            "Epoch 90/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 38.1459 - val_loss: 52.7660\n",
            "Epoch 91/200\n",
            "19/19 [==============================] - 2s 112ms/step - loss: 35.6067 - val_loss: 48.0180\n",
            "Epoch 92/200\n",
            "19/19 [==============================] - 3s 125ms/step - loss: 34.6121 - val_loss: 48.8171\n",
            "Epoch 93/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 32.4504 - val_loss: 60.3063\n",
            "Epoch 94/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 32.2923 - val_loss: 43.0968\n",
            "Epoch 95/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 30.3502 - val_loss: 41.3150\n",
            "Epoch 96/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 29.3784 - val_loss: 41.6308\n",
            "Epoch 97/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 28.6601 - val_loss: 41.7249\n",
            "Epoch 98/200\n",
            "19/19 [==============================] - 2s 111ms/step - loss: 28.1648 - val_loss: 40.6978\n",
            "Epoch 99/200\n",
            "19/19 [==============================] - 2s 128ms/step - loss: 27.2893 - val_loss: 39.1462\n",
            "Epoch 100/200\n",
            "19/19 [==============================] - 2s 88ms/step - loss: 26.3976 - val_loss: 38.0576\n",
            "Epoch 101/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 25.8132 - val_loss: 37.1746\n",
            "Epoch 102/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 25.0596 - val_loss: 36.3088\n",
            "Epoch 103/200\n",
            "19/19 [==============================] - 2s 80ms/step - loss: 24.5092 - val_loss: 35.6229\n",
            "Epoch 104/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 24.0070 - val_loss: 35.5590\n",
            "Epoch 105/200\n",
            "19/19 [==============================] - 2s 103ms/step - loss: 23.4154 - val_loss: 33.4910\n",
            "Epoch 106/200\n",
            "19/19 [==============================] - 2s 127ms/step - loss: 22.9539 - val_loss: 33.3071\n",
            "Epoch 107/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 22.5171 - val_loss: 32.6761\n",
            "Epoch 108/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 22.0787 - val_loss: 33.4991\n",
            "Epoch 109/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 21.7182 - val_loss: 31.1721\n",
            "Epoch 110/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 21.2039 - val_loss: 31.4655\n",
            "Epoch 111/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 20.7576 - val_loss: 30.3039\n",
            "Epoch 112/200\n",
            "19/19 [==============================] - 2s 95ms/step - loss: 20.3743 - val_loss: 29.6526\n",
            "Epoch 113/200\n",
            "19/19 [==============================] - 3s 134ms/step - loss: 20.0413 - val_loss: 28.8852\n",
            "Epoch 114/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 19.6119 - val_loss: 29.4168\n",
            "Epoch 115/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 19.3124 - val_loss: 27.9732\n",
            "Epoch 116/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 18.9359 - val_loss: 27.8437\n",
            "Epoch 117/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 18.5500 - val_loss: 27.5454\n",
            "Epoch 118/200\n",
            "19/19 [==============================] - 2s 88ms/step - loss: 18.2255 - val_loss: 26.2136\n",
            "Epoch 119/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 18.3090 - val_loss: 27.0824\n",
            "Epoch 120/200\n",
            "19/19 [==============================] - 3s 156ms/step - loss: 18.5100 - val_loss: 26.3678\n",
            "Epoch 121/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 17.7379 - val_loss: 25.9088\n",
            "Epoch 122/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 17.5260 - val_loss: 25.8413\n",
            "Epoch 123/200\n",
            "19/19 [==============================] - 2s 81ms/step - loss: 17.0111 - val_loss: 25.2066\n",
            "Epoch 124/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 16.7178 - val_loss: 24.4163\n",
            "Epoch 125/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 16.4662 - val_loss: 24.2851\n",
            "Epoch 126/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 16.2033 - val_loss: 24.4268\n",
            "Epoch 127/200\n",
            "19/19 [==============================] - 2s 134ms/step - loss: 15.8668 - val_loss: 23.9688\n",
            "Epoch 128/200\n",
            "19/19 [==============================] - 2s 89ms/step - loss: 15.5552 - val_loss: 23.7233\n",
            "Epoch 129/200\n",
            "19/19 [==============================] - 2s 91ms/step - loss: 15.2573 - val_loss: 23.8422\n",
            "Epoch 130/200\n",
            "19/19 [==============================] - 3s 142ms/step - loss: 15.1139 - val_loss: 23.1204\n",
            "Epoch 131/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 14.9372 - val_loss: 22.9288\n",
            "Epoch 132/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 14.9826 - val_loss: 22.9294\n",
            "Epoch 133/200\n",
            "19/19 [==============================] - 2s 110ms/step - loss: 14.5450 - val_loss: 22.3671\n",
            "Epoch 134/200\n",
            "19/19 [==============================] - 2s 114ms/step - loss: 14.3021 - val_loss: 22.4749\n",
            "Epoch 135/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 14.0501 - val_loss: 21.8968\n",
            "Epoch 136/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 13.9712 - val_loss: 22.0531\n",
            "Epoch 137/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 13.6779 - val_loss: 21.0479\n",
            "Epoch 138/200\n",
            "19/19 [==============================] - 2s 89ms/step - loss: 13.5753 - val_loss: 20.0538\n",
            "Epoch 139/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 13.3704 - val_loss: 20.5671\n",
            "Epoch 140/200\n",
            "19/19 [==============================] - 2s 107ms/step - loss: 13.2245 - val_loss: 20.6488\n",
            "Epoch 141/200\n",
            "19/19 [==============================] - 2s 126ms/step - loss: 13.0907 - val_loss: 20.2109\n",
            "Epoch 142/200\n",
            "19/19 [==============================] - 2s 89ms/step - loss: 12.8091 - val_loss: 19.2753\n",
            "Epoch 143/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 12.6316 - val_loss: 19.2558\n",
            "Epoch 144/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 12.5016 - val_loss: 19.3182\n",
            "Epoch 145/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 12.3492 - val_loss: 19.8480\n",
            "Epoch 146/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 12.2920 - val_loss: 18.9902\n",
            "Epoch 147/200\n",
            "19/19 [==============================] - 2s 99ms/step - loss: 12.1411 - val_loss: 18.6882\n",
            "Epoch 148/200\n",
            "19/19 [==============================] - 2s 126ms/step - loss: 11.8737 - val_loss: 18.4597\n",
            "Epoch 149/200\n",
            "19/19 [==============================] - 2s 90ms/step - loss: 11.7066 - val_loss: 18.9741\n",
            "Epoch 150/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 11.5900 - val_loss: 18.6722\n",
            "Epoch 151/200\n",
            "19/19 [==============================] - 2s 88ms/step - loss: 14.5857 - val_loss: 25.8348\n",
            "Epoch 152/200\n",
            "19/19 [==============================] - 2s 91ms/step - loss: 34.5847 - val_loss: 36.3163\n",
            "Epoch 153/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 29.8193 - val_loss: 49.7469\n",
            "Epoch 154/200\n",
            "19/19 [==============================] - 2s 108ms/step - loss: 63.6569 - val_loss: 64.7979\n",
            "Epoch 155/200\n",
            "19/19 [==============================] - 2s 119ms/step - loss: 32.7019 - val_loss: 31.6648\n",
            "Epoch 156/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 21.1817 - val_loss: 21.0565\n",
            "Epoch 157/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 16.8766 - val_loss: 19.4686\n",
            "Epoch 158/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 17.2437 - val_loss: 21.8886\n",
            "Epoch 159/200\n",
            "19/19 [==============================] - 2s 83ms/step - loss: 14.5196 - val_loss: 22.4305\n",
            "Epoch 160/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 13.8929 - val_loss: 20.8317\n",
            "Epoch 161/200\n",
            "19/19 [==============================] - 2s 102ms/step - loss: 12.9775 - val_loss: 20.0784\n",
            "Epoch 162/200\n",
            "19/19 [==============================] - 2s 130ms/step - loss: 12.0730 - val_loss: 21.1668\n",
            "Epoch 163/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 12.1218 - val_loss: 19.3678\n",
            "Epoch 164/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 11.8818 - val_loss: 19.0200\n",
            "Epoch 165/200\n",
            "19/19 [==============================] - 2s 81ms/step - loss: 21.4731 - val_loss: 38.8695\n",
            "Epoch 166/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 22.4226 - val_loss: 22.0255\n",
            "Epoch 167/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 14.1508 - val_loss: 19.1761\n",
            "Epoch 168/200\n",
            "19/19 [==============================] - 2s 99ms/step - loss: 13.1100 - val_loss: 17.5931\n",
            "Epoch 169/200\n",
            "19/19 [==============================] - 3s 132ms/step - loss: 12.7272 - val_loss: 16.4295\n",
            "Epoch 170/200\n",
            "19/19 [==============================] - 2s 88ms/step - loss: 11.3566 - val_loss: 16.1613\n",
            "Epoch 171/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 10.9537 - val_loss: 16.0455\n",
            "Epoch 172/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 10.6686 - val_loss: 15.8392\n",
            "Epoch 173/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 10.3983 - val_loss: 16.3569\n",
            "Epoch 174/200\n",
            "19/19 [==============================] - 2s 88ms/step - loss: 10.5215 - val_loss: 14.4354\n",
            "Epoch 175/200\n",
            "19/19 [==============================] - 2s 97ms/step - loss: 11.0243 - val_loss: 17.4361\n",
            "Epoch 176/200\n",
            "19/19 [==============================] - 3s 131ms/step - loss: 11.6049 - val_loss: 17.3763\n",
            "Epoch 177/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 10.1941 - val_loss: 17.6257\n",
            "Epoch 178/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 9.7693 - val_loss: 17.1349\n",
            "Epoch 179/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 9.6446 - val_loss: 16.8950\n",
            "Epoch 180/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 9.4639 - val_loss: 17.0143\n",
            "Epoch 181/200\n",
            "19/19 [==============================] - 2s 89ms/step - loss: 9.3725 - val_loss: 16.9097\n",
            "Epoch 182/200\n",
            "19/19 [==============================] - 2s 93ms/step - loss: 9.3597 - val_loss: 16.7570\n",
            "Epoch 183/200\n",
            "19/19 [==============================] - 3s 141ms/step - loss: 9.2219 - val_loss: 16.8187\n",
            "Epoch 184/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 9.1301 - val_loss: 16.8095\n",
            "Epoch 185/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 9.1930 - val_loss: 16.5857\n",
            "Epoch 186/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 9.6675 - val_loss: 13.7098\n",
            "Epoch 187/200\n",
            "19/19 [==============================] - 2s 81ms/step - loss: 9.1658 - val_loss: 17.1363\n",
            "Epoch 188/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 9.1566 - val_loss: 19.2295\n",
            "Epoch 189/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 8.7613 - val_loss: 16.7345\n",
            "Epoch 190/200\n",
            "19/19 [==============================] - 3s 150ms/step - loss: 8.5267 - val_loss: 16.1520\n",
            "Epoch 191/200\n",
            "19/19 [==============================] - 2s 88ms/step - loss: 8.6239 - val_loss: 13.5071\n",
            "Epoch 192/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 9.6023 - val_loss: 13.7577\n",
            "Epoch 193/200\n",
            "19/19 [==============================] - 2s 88ms/step - loss: 9.6471 - val_loss: 14.9070\n",
            "Epoch 194/200\n",
            "19/19 [==============================] - 2s 87ms/step - loss: 8.5128 - val_loss: 13.5242\n",
            "Epoch 195/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 8.2032 - val_loss: 13.1764\n",
            "Epoch 196/200\n",
            "19/19 [==============================] - 2s 84ms/step - loss: 8.1530 - val_loss: 13.0260\n",
            "Epoch 197/200\n",
            "19/19 [==============================] - 3s 140ms/step - loss: 8.0405 - val_loss: 13.2612\n",
            "Epoch 198/200\n",
            "19/19 [==============================] - 2s 82ms/step - loss: 8.5259 - val_loss: 15.2704\n",
            "Epoch 199/200\n",
            "19/19 [==============================] - 2s 86ms/step - loss: 10.6075 - val_loss: 16.1280\n",
            "Epoch 200/200\n",
            "19/19 [==============================] - 2s 85ms/step - loss: 9.4576 - val_loss: 14.1721\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Dense, Dropout, Input, Concatenate\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Load the data from the JSON file\n",
        "with open('/content/sample_data/realistic_trajectory_data-24.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# Extract the trajectories and initial accelerations from the data\n",
        "trajectories = []\n",
        "initial_accs = []\n",
        "for point in data:\n",
        "    trajectories.append([point['x'], point['y'], point['z']])\n",
        "    initial_accs.append([point['ax'], point['ay'], point['az']])\n",
        "\n",
        "# Define the number of timesteps and features\n",
        "num_timesteps = 13  # Number of previous trajectory points to consider\n",
        "num_pos_features = 3  # x, y, z coordinates\n",
        "\n",
        "# Prepare the training data\n",
        "X_train_pos = []\n",
        "X_train_acc = []\n",
        "y_train = []\n",
        "for i in range(len(trajectories) - num_timesteps):\n",
        "    X_train_pos.append(trajectories[i:i+num_timesteps])\n",
        "    X_train_acc.append(initial_accs[i])\n",
        "    y_train.append(trajectories[i + num_timesteps])\n",
        "\n",
        "# Convert to numpy arrays for training\n",
        "X_train_pos = np.array(X_train_pos)\n",
        "X_train_acc = np.array(X_train_acc)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "# Define the LSTM model\n",
        "input_pos = Input(shape=(num_timesteps, num_pos_features))\n",
        "input_acc = Input(shape=(3,))\n",
        "\n",
        "x = LSTM(128, input_shape=(num_timesteps, num_pos_features), return_sequences=True, kernel_regularizer=l2(0.01))(input_pos)\n",
        "x = Dropout(0.2)(x)\n",
        "x = LSTM(128, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = LSTM(64, kernel_regularizer=l2(0.01))(x)\n",
        "combined = Concatenate(axis=-1)([x, input_acc])\n",
        "output = Dense(num_pos_features, kernel_regularizer=l2(0.01))(combined)\n",
        "\n",
        "model = Model(inputs=[input_pos, input_acc], outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit([X_train_pos, X_train_acc], y_train, epochs=200, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('trajectory_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('trajectory_model.h5')\n",
        "\n",
        "# Initial position and acceleration\n",
        "initial_pos = np.array([3.075831475154961, 4.206734691502548, 2.950718916245421])\n",
        "initial_acc = np.array([1046.9889099743164, 1431.9394949898867, 1057.642122414968])\n",
        "\n",
        "# Prepare the initial sequence with the single initial position and constant acceleration\n",
        "# Assuming the model was trained with sequences where acceleration is constant and only the positions vary\n",
        "input_seq_pos = [initial_pos] * num_timesteps  # Replicate the initial position to fill the sequence\n",
        "input_seq_acc = initial_acc.reshape(1, 3)  # Keep the acceleration constant\n",
        "\n",
        "predicted_trajectory = []\n",
        "\n",
        "# Predict the next 5 positions\n",
        "for _ in range(8):\n",
        "    X_test_pos = np.array(input_seq_pos[-num_timesteps:]).reshape(1, num_timesteps, num_pos_features)  # Use the most recent positions\n",
        "    next_pos = model.predict([X_test_pos, input_seq_acc])[0]  # Predict the next position\n",
        "\n",
        "    # Update the input sequence and store the prediction\n",
        "    input_seq_pos.append(next_pos)  # Add the predicted position to the input sequence\n",
        "    predicted_trajectory.append(next_pos)  # Store the prediction\n",
        "\n",
        "print(\"Initial Position:\", initial_pos.tolist())\n",
        "print(\"Initial Acceleration:\", initial_acc.tolist())\n",
        "print(\"Predicted Trajectory:\")\n",
        "for pos in predicted_trajectory:\n",
        "    print(pos.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpvSvgkbc_HZ",
        "outputId": "ea9109a4-23cf-47fb-8cc1-3547a9a80968"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "Initial Position: [3.075831475154961, 4.206734691502548, 2.950718916245421]\n",
            "Initial Acceleration: [1046.9889099743164, 1431.9394949898867, 1057.642122414968]\n",
            "Predicted Trajectory:\n",
            "[22.45094108581543, 22.698659896850586, 5.930832862854004]\n",
            "[22.213090896606445, 22.464784622192383, 5.763583183288574]\n",
            "[21.93588638305664, 22.148523330688477, 5.5632100105285645]\n",
            "[21.910865783691406, 21.918987274169922, 5.323182106018066]\n",
            "[22.653284072875977, 22.06784439086914, 5.012879371643066]\n",
            "[25.961118698120117, 23.92301368713379, 4.681106090545654]\n",
            "[32.72053146362305, 28.896780014038086, 4.372860908508301]\n",
            "[40.161495208740234, 35.70177459716797, 3.5946357250213623]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_VSntagpRkLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7a365a-0e15-4df6-a26b-9fd246d325ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "Initial Position:  [3.075831475154961, 4.206734691502548, 2.950718916245421]\n",
            "Initial Acceleration:  [1046.9889099743164, 1431.9394949898867, 1057.642122414968]\n",
            "Predicted Trajectory:\n",
            "[3.075831475154961, 4.206734691502548, 2.950718916245421]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[34.862854  38.91199    3.6628804]\n",
            "[ 2.0231404 -2.9881165  2.877731 ]\n",
            "[-0.3631493 -1.290706   3.1133587]\n",
            "[4.7582955 0.3470046 5.7173424]\n",
            "[9.015968  2.4612586 8.86975  ]\n",
            "[12.936681   3.8546302 11.181785 ]\n",
            "[16.987772   4.8201756 13.211766 ]\n",
            "[21.324932   5.8025746 14.928819 ]\n",
            "[26.207426  7.222713 16.638325]\n",
            "[30.071295  8.617672 17.892376]\n",
            "[33.44867   9.949028 18.827126]\n",
            "[37.121674 11.297537 19.591103]\n",
            "[40.090866 11.996479 19.851753]\n",
            "[40.282497 11.328035 19.432426]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout, Input, Concatenate\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('trajectory_model.h5')\n",
        "\n",
        "# Define the number of timesteps and features\n",
        "num_timesteps = 13 # Number of previous trajectory points to consider\n",
        "num_pos_features = 3  # x, y, z coordinates\n",
        "\n",
        "# Test the model with a random initial position and acceleration\n",
        "initial_pos = [3.075831475154961,4.206734691502548,2.950718916245421]\n",
        "initial_acc = [1046.9889099743164,1431.9394949898867,1057.642122414968]\n",
        "input_seq = [initial_pos]\n",
        "predicted_trajectory = [initial_pos]\n",
        "\n",
        "for _ in range(num_timesteps - 1):\n",
        "    X_pos = np.array(input_seq[-num_timesteps:]) if len(input_seq) >= num_timesteps else np.zeros((num_timesteps, num_pos_features))\n",
        "    X_test_pos = X_pos.reshape(1, num_timesteps, num_pos_features)\n",
        "    X_test_acc = np.array(initial_acc).reshape(1, 3)\n",
        "    next_pos = model.predict([X_test_pos, X_test_acc])\n",
        "    input_seq.append(next_pos[0])\n",
        "    predicted_trajectory.append(next_pos[0])\n",
        "\n",
        "# Predict the next 5 positions\n",
        "for _ in range(13):\n",
        "    X_pos = np.array(input_seq[-num_timesteps:])\n",
        "    X_test_pos = X_pos.reshape(1, num_timesteps, num_pos_features)\n",
        "    X_test_acc = np.array(initial_acc).reshape(1, 3)\n",
        "    next_pos = model.predict([X_test_pos, X_test_acc])\n",
        "    input_seq.append(next_pos[0])\n",
        "    predicted_trajectory.append(next_pos[0])\n",
        "\n",
        "print(\"Initial Position: \", initial_pos)\n",
        "print(\"Initial Acceleration: \", initial_acc)\n",
        "print(\"Predicted Trajectory:\")\n",
        "for pos in predicted_trajectory:\n",
        "    print(pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcl48EpNavs4"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Load the previously trained model\n",
        "model = load_model('trajectory_model.h5')\n",
        "\n",
        "# Load new data from another JSON file\n",
        "with open('/content/sample_data/realistic_trajectory_data-12.json', 'r') as file:\n",
        "    new_data = json.load(file)\n",
        "\n",
        "# Extract trajectories and initial accelerations from the new data\n",
        "new_trajectories = []\n",
        "new_initial_accs = []\n",
        "for point in new_data:\n",
        "    new_trajectories.append([point['x'], point['y'], point['z']])\n",
        "    new_initial_accs.append([point['ax'], point['ay'], point['az']])\n",
        "\n",
        "# Prepare the new training data\n",
        "X_train_pos_new = []\n",
        "X_train_acc_new = []\n",
        "y_train_new = []\n",
        "num_timesteps = 5  # This should match the value used during initial training\n",
        "for i in range(len(new_trajectories) - num_timesteps):\n",
        "    X_train_pos_new.append(new_trajectories[i:i+num_timesteps])\n",
        "    X_train_acc_new.append(new_initial_accs[i])\n",
        "    y_train_new.append(new_trajectories[i + num_timesteps])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train_pos_new = np.array(X_train_pos_new)\n",
        "X_train_acc_new = np.array(X_train_acc_new)\n",
        "y_train_new = np.array(y_train_new)\n",
        "\n",
        "# Continue training the model on the new data\n",
        "model.fit([X_train_pos_new, X_train_acc_new], y_train_new, epochs=200, batch_size=64, validation_split=0.2)\n",
        "\n",
        "# Optionally save the updated model\n",
        "model.save('updated_trajectory_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB-LBmLrgHg9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import LSTM, Dense, Dropout, Input, Concatenate\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('updated_trajectory_model.h5')\n",
        "\n",
        "# Define the number of timesteps and features\n",
        "num_timesteps = 5  # Number of previous trajectory points to consider\n",
        "num_pos_features = 3  # x, y, z coordinates\n",
        "\n",
        "# Test the model with a random initial position and acceleration\n",
        "initial_pos = [19.893560441900362,11.676007613329734,12.512333688615755]\n",
        "initial_acc = [1612.2885586899642,946.2908131064622,1237.6817826067672]\n",
        "input_seq = [initial_pos]\n",
        "predicted_trajectory = [initial_pos]\n",
        "\n",
        "for _ in range(num_timesteps - 1):\n",
        "    X_pos = np.array(input_seq[-num_timesteps:]) if len(input_seq) >= num_timesteps else np.zeros((num_timesteps, num_pos_features))\n",
        "    X_test_pos = X_pos.reshape(1, num_timesteps, num_pos_features)\n",
        "    X_test_acc = np.array(initial_acc).reshape(1, 3)\n",
        "    next_pos = model.predict([X_test_pos, X_test_acc])\n",
        "    input_seq.append(next_pos[0])\n",
        "    predicted_trajectory.append(next_pos[0])\n",
        "\n",
        "# Predict the next 5 positions\n",
        "for _ in range(5):\n",
        "    X_pos = np.array(input_seq[-num_timesteps:])\n",
        "    X_test_pos = X_pos.reshape(1, num_timesteps, num_pos_features)\n",
        "    X_test_acc = np.array(initial_acc).reshape(1, 3)\n",
        "    next_pos = model.predict([X_test_pos, X_test_acc])\n",
        "    input_seq.append(next_pos[0])\n",
        "    predicted_trajectory.append(next_pos[0])\n",
        "\n",
        "print(\"Initial Position: \", initial_pos)\n",
        "print(\"Initial Acceleration: \", initial_acc)\n",
        "print(\"Predicted Trajectory:\")\n",
        "for pos in predicted_trajectory:\n",
        "    print(pos)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5cVRBv8P0eCGM3ABevwAY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}